{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load credentials from .env\n",
    "load_dotenv()\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def get_episode_embeddings(driver):\n",
    "    episode_embeddings = {}\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e:Episode)-[:HAS_SEGMENT]->(s:TranscriptSegment)\n",
    "            RETURN e.episode_number AS episode, s.embedding AS embedding\n",
    "        \"\"\")\n",
    "        for record in result:\n",
    "            ep = record['episode']\n",
    "            emb = record['embedding']\n",
    "            if emb is not None:\n",
    "                if ep not in episode_embeddings:\n",
    "                    episode_embeddings[ep] = []\n",
    "                episode_embeddings[ep].append(emb)\n",
    "    # Compute average embedding for each episode, only if there are valid embeddings\n",
    "    avg_embeddings = {}\n",
    "    for ep, embs in episode_embeddings.items():\n",
    "        valid_embs = [e for e in embs if e is not None]\n",
    "        if valid_embs:\n",
    "            avg_embeddings[ep] = np.mean(np.array(valid_embs), axis=0)\n",
    "    return avg_embeddings\n",
    "\n",
    "avg_embeddings = get_episode_embeddings(driver)\n",
    "\n",
    "\n",
    "def find_most_similar_episodes(target_episode, avg_embeddings, top_n=5):\n",
    "    target_emb = avg_embeddings[target_episode].reshape(1, -1)\n",
    "    all_eps = list(avg_embeddings.keys())\n",
    "    all_embs = np.stack([avg_embeddings[ep] for ep in all_eps])\n",
    "    sims = cosine_similarity(target_emb, all_embs)[0]\n",
    "    sim_scores = list(zip(all_eps, sims))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: -x[1])\n",
    "    # Exclude the episode itself\n",
    "    sim_scores = [s for s in sim_scores if s[0] != target_episode]\n",
    "    return sim_scores[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar episodes to 437:\n",
      "Episode 438: similarity 0.973\n",
      "Episode 140: similarity 0.968\n",
      "Episode 426: similarity 0.966\n",
      "Episode 323: similarity 0.965\n",
      "Episode 259: similarity 0.964\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "target_episode = 437  # Change as needed\n",
    "similar_episodes = find_most_similar_episodes(target_episode, avg_embeddings)\n",
    "print(f\"Most similar episodes to {target_episode}:\")\n",
    "for ep, score in similar_episodes:\n",
    "    print(f\"Episode {ep}: similarity {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "episodes_df = pd.read_csv(r'G:\\My Drive\\Projects\\naruhodo_references\\references_Link\\Podcast_Neo4j\\data\\processed\\naruhodo_episodes.csv')\n",
    "\n",
    "# Inspect the columns to confirm names (uncomment to check)\n",
    "# print(episodes_df.columns)\n",
    "\n",
    "# Build a mapping from episode number to title\n",
    "episode_to_title = pd.Series(episodes_df['episode_title'].values, index=episodes_df['episode_number']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2bf33f30b84fb08a13a2dd229dcca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Episode:', options=(1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_similar_episodes(target_episode)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def show_similar_episodes(target_episode):\n",
    "    selected_title = episode_to_title.get(target_episode, \"Unknown Title\")\n",
    "    print(f\"\\nSelected episode: {target_episode} | Title: {selected_title}\\n\")\n",
    "    similar_episodes = find_most_similar_episodes(target_episode, avg_embeddings)\n",
    "    print(\"Most similar episodes:\")\n",
    "    for ep, score in similar_episodes:\n",
    "        title = episode_to_title.get(ep, \"Unknown Title\")\n",
    "        print(f\"Episode {ep}: similarity {score:.3f} | Title: {title}\")\n",
    "\n",
    "episode_selector = widgets.Dropdown(\n",
    "    options=sorted(avg_embeddings.keys()),\n",
    "    description='Episode:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "widgets.interact(show_similar_episodes, target_episode=episode_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Cosine Similarity Works**\n",
    "\n",
    "**Definition:**  \n",
    "- Cosine similarity measures the **cosine of the angle** between two vectors in a multi-dimensional space.\n",
    "\n",
    "**Range:**  \n",
    "- **+1**: Vectors point in exactly the **same direction** (most similar).  \n",
    "- **0**: Vectors are **orthogonal** (no similarity).  \n",
    "- **-1**: Vectors point in **opposite directions** (most dissimilar; rare in embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "**Mathematical Formula**  \n",
    "Given two vectors **A** and **B**:\n",
    "\n",
    "cosine_similarity = (A · B) / (||A|| × ||B||)\n",
    "\n",
    "\n",
    "- **A · B** is the **dot product** of the vectors.  \n",
    "- **||A||** and **||B||** are the **magnitudes (lengths)** of the vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use Cosine Similarity for Embeddings?**  \n",
    "- Embeddings (like your **episode vectors**) capture the **“meaning”** or **content** of each episode in high-dimensional space.  \n",
    "- Cosine similarity tells you how close the **direction** of two episodes is, reflecting how **similar their content** is, regardless of their length.  \n",
    "- It’s **robust to differences in scale** and especially useful for **text and semantic data**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example**  \n",
    "Suppose you have two episode embeddings:\n",
    "\n",
    "- **Episode A**: `[0.1, 0.2, 0.3]`  \n",
    "- **Episode B**: `[0.2, 0.4, 0.6]`  \n",
    "\n",
    "The cosine similarity is:\n",
    "\n",
    "\n",
    "(\n",
    "0.1\n",
    "∗\n",
    "0.2\n",
    "+\n",
    "0.2\n",
    "∗\n",
    "0.4\n",
    "+\n",
    "0.3\n",
    "∗\n",
    "0.6\n",
    ")\n",
    "0.\n",
    "1\n",
    "2\n",
    "+\n",
    "0.\n",
    "2\n",
    "2\n",
    "+\n",
    "0.\n",
    "3\n",
    "2\n",
    "×\n",
    "0.\n",
    "2\n",
    "2\n",
    "+\n",
    "0.\n",
    "4\n",
    "2\n",
    "+\n",
    "0.\n",
    "6\n",
    "2\n",
    "=\n",
    "1\n",
    "0.1 \n",
    "2\n",
    " +0.2 \n",
    "2\n",
    " +0.3 \n",
    "2\n",
    " \n",
    "​\n",
    " × \n",
    "0.2 \n",
    "2\n",
    " +0.4 \n",
    "2\n",
    " +0.6 \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "(0.1∗0.2+0.2∗0.4+0.3∗0.6)\n",
    "​\n",
    " =1\n",
    "\n",
    "\n",
    "Since **B is just a scaled version of A**, they are **perfectly similar** (cosine similarity = 1).\n",
    "\n",
    "---\n",
    "\n",
    "**In Practice:**  \n",
    "- **High cosine similarity (~1)**: Episodes are **very similar** in content.  \n",
    "- **Low cosine similarity (~0)**: Episodes are **unrelated**.  \n",
    "- **Negative values**: Rare in practice with embeddings, but would mean **opposite meanings**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Cosine Similarity | Interpretation                     |\n",
    "|-------------------|-------------------------------------|\n",
    "| 1                 | Identical direction (most similar)  |\n",
    "| 0                 | No similarity                       |\n",
    "| -1                | Opposite direction (most dissimilar) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Cosine Similarity Works**\n",
    "\n",
    "**Definition:**  \n",
    "- Cosine similarity measures the **cosine of the angle** between two vectors in a multi-dimensional space.\n",
    "\n",
    "**Range:**  \n",
    "- **+1**: Vectors point in exactly the **same direction** (most similar).  \n",
    "- **0**: Vectors are **orthogonal** (no similarity).  \n",
    "- **-1**: Vectors point in **opposite directions** (most dissimilar; rare in embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "**Mathematical Formula**  \n",
    "Given two vectors **A** and **B**:\n",
    "\n",
    "```\n",
    "cosine_similarity = (A · B) / (||A|| × ||B||)\n",
    "```\n",
    "\n",
    "- **A · B** is the **dot product** of the vectors.  \n",
    "- **||A||** and **||B||** are the **magnitudes (lengths)** of the vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use Cosine Similarity for Embeddings?**  \n",
    "- Embeddings (like your **episode vectors**) capture the **“meaning”** or **content** of each episode in high-dimensional space.  \n",
    "- Cosine similarity tells you how close the **direction** of two episodes is, reflecting how **similar their content** is, regardless of their length.  \n",
    "- It’s **robust to differences in scale** and especially useful for **text and semantic data**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example**  \n",
    "Suppose you have two episode embeddings:\n",
    "\n",
    "- **Episode A**: `[0.1, 0.2, 0.3]`  \n",
    "- **Episode B**: `[0.2, 0.4, 0.6]`  \n",
    "\n",
    "The cosine similarity is:\n",
    "\n",
    "```\n",
    "(0.1*0.2 + 0.2*0.4 + 0.3*0.6) / (sqrt(0.1² + 0.2² + 0.3²) × sqrt(0.2² + 0.4² + 0.6²)) = 1\n",
    "```\n",
    "\n",
    "Since **B is just a scaled version of A**, they are **perfectly similar** (cosine similarity = 1).\n",
    "\n",
    "---\n",
    "\n",
    "**In Practice:**  \n",
    "- **High cosine similarity (~1)**: Episodes are **very similar** in content.  \n",
    "- **Low cosine similarity (~0)**: Episodes are **unrelated**.  \n",
    "- **Negative values**: Rare in practice with embeddings, but would mean **opposite meanings**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Cosine Similarity | Interpretation                     |\n",
    "|-------------------|-------------------------------------|\n",
    "| 1                 | Identical direction (most similar)  |\n",
    "| 0                 | No similarity                       |\n",
    "| -1                | Opposite direction (most dissimilar) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the diferences between Cosine Similarity and GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine Similarity**\n",
    "\n",
    "**What it is:**  \n",
    "- A mathematical measure of similarity between two vectors, based on the angle between them.  \n",
    "- Commonly used to compare text/document embeddings, such as those from BERT, sentence-transformers, etc.\n",
    "\n",
    "**How it works:**  \n",
    "- Takes two vectors (e.g., episode embeddings) and computes the **cosine of the angle** between them.  \n",
    "- Values range from **-1** (opposite) to **1** (identical direction), with **0** meaning orthogonal (unrelated).\n",
    "\n",
    "**Use case:**  \n",
    "- Find the most similar items (episodes, documents, etc.) based on their content or semantic meaning.  \n",
    "- Simple, fast, and works well for **“nearest neighbor”** search in embedding space.\n",
    "\n",
    "**Example:**  \n",
    "- “Which episodes are most similar in content to episode 42?”\n",
    "\n",
    "---\n",
    "\n",
    "**GraphRAG (Graph Retrieval-Augmented Generation)**\n",
    "\n",
    "**What it is:**  \n",
    "- A framework that combines a **graph database** (like Neo4j) with a **large language model (LLM)** for advanced retrieval and generation.  \n",
    "- “RAG” stands for **Retrieval-Augmented Generation**: the LLM is given context retrieved from a knowledge base (in this case, a graph).\n",
    "\n",
    "**How it works:**  \n",
    "When you ask a question, GraphRAG:  \n",
    "1. Retrieves **relevant nodes/edges** from your graph (using embeddings, graph queries, or both).  \n",
    "2. Feeds this **structured and unstructured context** to the LLM.  \n",
    "3. The LLM generates an **answer, summary, or explanation**, grounded in both the graph’s structure and the content.\n",
    "\n",
    "**Use case:**  \n",
    "- Complex, relational, or **multi-hop questions** that require understanding both the content and the relationships between items.  \n",
    "- Summarization, Q&A, and **reasoning over knowledge graphs**.\n",
    "\n",
    "**Example:**  \n",
    "- “Which episodes discuss both neuroscience and education, and how are they connected?”  \n",
    "- “Summarize the main findings from all episodes that reference a specific paper.”\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Feature/Goal       | Cosine Similarity                  | GraphRAG                                      |\n",
    "|--------------------|------------------------------------|-----------------------------------------------|\n",
    "| Main purpose        | Find similar items by content      | Answer/generate using graph + LLM             |\n",
    "| Uses graph structure| No                                 | Yes                                           |\n",
    "| Uses LLM            | No                                 | Yes                                           |\n",
    "| Output              | List of similar items              | Answers, summaries, generated text            |\n",
    "| Complexity          | Simple, fast                       | More complex, powerful                        |\n",
    "| Example             | “Find similar episodes”            | “Explain how these episodes are related”      |\n",
    "\n",
    "---\n",
    "\n",
    "**When to Use Each**\n",
    "\n",
    "**Cosine similarity:**  \n",
    "- When you want to quickly **find similar items based on content/embeddings**.  \n",
    "- Ideal for **recommendations**, “related episodes,” or clustering.\n",
    "\n",
    "**GraphRAG:**  \n",
    "- When you want to **answer complex questions** that require both content and relationships.  \n",
    "- Best for **advanced Q&A, summarization**, or reasoning over your **knowledge graph**.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**  \n",
    "- **Cosine similarity** is a **simple, direct** way to measure content similarity.  \n",
    "- **GraphRAG** is a **powerful, LLM-driven** approach for leveraging both your **graph’s structure and content** for advanced retrieval and generation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
